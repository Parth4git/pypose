import torch as torch
import torch.nn as nn
import pypose as pp
from torch.autograd.functional import jacobian
from torch.autograd import grad


class Constraint(nn.Module):
    r'''
    The base class of a constraint function.
    
    todo: change description
    The cost function :math:`\mathrm{cons}` is given by:

    Example:

    Note:
        More practical examples can be found at `examples/module/constraint
        <https://github.com/pypose/pypose/tree/main/examples/module/constraint>`_.
    '''

    def __init__(self):
        super().__init__()
        self.jacargs = {'vectorize':True, 'strategy':'reverse-mode'}
        # self.register_buffer('_t',torch.zeros(1))
        # self.register_forward_hook(self.forward_hook)

    # def forward_hook(self, module, inputs, outputs):
    #     r'''
    #     Automatically advances the time step.
    #     '''
    #     self._t.add_(1)

    def forward(self, state, input):
        r'''
        Defines the computation performed at every call that advances the system by one time step.

        Note:
            The :obj:`forward` method implicitly increments the time step via :obj:`forward_hook`.
            :obj:`state_transition` and :obj:`observation` still accept time for the flexiblity
            such as time-varying system. One can directly access the current system time via the
            property :obj:`systime`.
        '''
        self.state, self.input = torch.atleast_1d(state), torch.atleast_1d(input)
        return self.constraint(self.state, self.input)

    def constraint(self, state, input, t=None):
        r'''
        Args:
            state (:obj:`Tensor`): The state of the dynamical system
            input (:obj:`Tensor`): The input to the dynamical system
            t (:obj:`Tensor`): The time step of the dynamical system.  Default: ``None``.

        Returns:
            Tensor: The state of the system at next time step

        Note:
            The users need to define this method and can access the current time via the property
            :obj:`systime`.
        '''
        raise NotImplementedError("The users need to define their own state transition method")

    # def reset(self, t=0):
    #     self._t.fill_(t)

    def set_refpoint(self, state=None, input=None):
        r'''
        Function to set the reference point for linearization.

        Args: 
            state (:obj:`Tensor`): The reference state of the dynamical system. If ``None``,
                the the most recent state is taken. Default: ``None``.
            input (:obj:`Tensor`): The reference input to the dynamical system. If ``None``,
                the the most recent input is taken. Default: ``None``.
            t (:obj:`Tensor`): The reference time step of the dynamical system. If ``None``,
                the the most recent timestamp is taken. Default: ``None``.

        Returns:
            None

        Warning:
            For nonlinear systems, the users have to call this function before getting the
            linearized system.
        '''
        self._ref_state = torch.tensor(self.state) if state is None else torch.atleast_1d(state)
        self._ref_input = torch.tensor(self.input) if input is None else torch.atleast_1d(input)
        self._ref_c = self.constraint(self._ref_state, self._ref_input)

    # @property
    # def systime(self):
    #     r'''
    #         System time, automatically advanced by :obj:`forward_hook`.
    #     '''
    #     return self._t

    @property
    def gx(self):
        r'''
        Quadratic/quadraticized cost linear term on state
        
        todo: change this
        .. math::
            \mathbf{B} = \left. \frac{\partial \mathbf{f}}{\partial \mathbf{u}} \right|_{\chi^*}
        '''
        func = lambda x: self.constraint(x, self._ref_input)
        return jacobian(func, self._ref_state, **self.jacargs)

    @property
    def gu(self):
        r'''
        Quadratic/quadraticized cost linear term on input
        
        todo: change this
        .. math::
            \mathbf{B} = \left. \frac{\partial \mathbf{f}}{\partial \mathbf{u}} \right|_{\chi^*}
        '''
        func = lambda x: self.cost(self._ref_state, x)
        return jacobian(func, self._ref_input, **self.jacargs)    

    @property
    def c(self):
        r'''
        Constant term generated by cost.

        .. math::
            \mathbf{c}_1 = \mathbf{f}(\mathbf{x}^*, \mathbf{u}^*, t^*)
                           - \mathbf{A}\mathbf{x}^* - \mathbf{B}\mathbf{u}^*
        '''
        # Potential performance loss here - self.A and self.B involves jacobian eval
        return self._ref_c - self._ref_state.matmul(self.gx.mT) - self._ref_input.matmul(self.gu.mT)


class LinCon(Constraint):
    r'''
    Quadratic cost.
    
    todo: change
    Args:
        A (:obj:`Tensor`): The state matrix of LTI system.
        B (:obj:`Tensor`): The input matrix of LTI system.
        C (:obj:`Tensor`): The output matrix of LTI system.
        D (:obj:`Tensor`): The observation matrix of LTI system,
        c1 (:obj:`Tensor`): The constant input of LTI system,
        c2 (:obj:`Tensor`): The constant output of LTI system.

    A linear time-invariant lumped system can be described by state-space equation of the form:

    .. math::
        \begin{align*}
            \mathbf{z} = \mathbf{A}\mathbf{x} + \mathbf{B}\mathbf{u} + \mathbf{c}_1 \\
            \mathbf{y} = \mathbf{C}\mathbf{x} + \mathbf{D}\mathbf{u} + \mathbf{c}_2 \\
        \end{align*}

    where :math:`\mathbf{x}` and :math:`\mathbf{u}` are state and input of the current
    timestamp of LTI system.

    Note:
        The variables including state and input are row vectors, which is the last dimension of
        a Tensor. :obj:`A`, :obj:`B`, :obj:`C`, :obj:`D`, :obj:`x`, :obj:`u` could be a single
        matrix or batched matrices. In the batch case, their dimensions must be consistent so that
        they can be multiplied for each channel.

    Example:
        >>> A = torch.randn(2, 3, 3)
        >>> B = torch.randn(2, 3, 2)
        >>> C = torch.randn(2, 3, 3)
        >>> D = torch.randn(2, 3, 2)
        >>> c1 = torch.randn(2, 1, 3)
        >>> c2 = torch.randn(2, 1, 3)
        >>> state = torch.randn(2, 1, 3)
        >>> input = torch.randn(2, 1, 2)
        >>> lti = pp.module.LTI(A, B, C, D, c1, c2)
        >>> lti(state, input)
        tensor([[[-8.5639,  0.0523, -0.2576]],
                [[ 4.1013, -1.5452, -0.0233]]]), 
        tensor([[[-3.5780, -2.2970, -2.9314]], 
                [[-0.4358,  1.7306,  2.7514]]]))

    Note:
        In this general example, all variables are in a batch. User definable as appropriate.

    Note:
        More practical examples can be found at `examples/module/dynamics
        <https://github.com/pypose/pypose/tree/main/examples/module/dynamics>`_.
    '''
    
    def __init__(self, gx, gu, g=None):
        super(LinCon, self).__init__()
        assert gx.ndim in (2, 3), "Invalid cost state Matrices dimensions" 
        assert gx.ndim == gu.ndim, "Invalid System Matrices dimensions"
        self.gx, self.gu, self.g = gx, gu, g

    def forward(self, state, input):
        r'''
        Perform one step advance for the quadratic cost.

        '''
        if self.gx.ndim >= 3:
            assert self.gx.ndim == state.ndim == input.ndim,  "Invalid System Matrices dimensions"

        return super(LinCon, self).forward(state, input)

    def constraint(self, state, input):
        r'''
        Perform one step of LTI state transition.

        .. math::
            \mathbf{z} = \mathbf{A}\mathbf{x} + \mathbf{B}\mathbf{u} + \mathbf{c}_1 \\

        '''
        # print('checkpoint', state.matmul(self.cxx.mT).size() )
        # print(state.matmul(self.cx.mT).size())
        # exit()
        return state.matmul(self.gx.mT) + input.matmul(self.gu.mT) \
                        + self.g

    @property
    def gx(self):
        r'''
        System state matrix :obj:`gx`
        '''
        return self._gx

    @gx.setter
    def gx(self, gx):
        self._gx = gx

    @property
    def gu(self):
        r'''
        System input matrix :obj:`gu`
        '''
        return self._gu

    @gu.setter
    def gu(self, gu):
        self._gu = gu

    @property
    def g(self):
        r'''
        System output matrix :obj:`g`
        '''
        return self._g

    @g.setter
    def g(self, g):
        self._g = g

if __name__ == "__main__":
        gx = torch.randn(2, 3)
        gu = torch.randn(2, 2)
        g = torch.randn(2, 1)
        state = torch.randn(1, 3)
        input = torch.randn(1, 2)
        lincon = pp.module.LinCon(gx,gu,g)
        print(lincon(state, input))
        print(lincon.gx)